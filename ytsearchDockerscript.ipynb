{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d949f440-507f-4430-a82f-23e975d01579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (1.34.49)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.49 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from boto3) (1.34.49)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from botocore<1.35.0,>=1.34.49->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from botocore<1.35.0,>=1.34.49->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.49->boto3) (1.16.0)\n",
      "Requirement already satisfied: yt_dlp in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (2023.12.30)\n",
      "Requirement already satisfied: mutagen in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (3.20.0)\n",
      "Requirement already satisfied: certifi in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (2024.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.31.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.17 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (1.26.18)\n",
      "Requirement already satisfied: websockets>=12.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (12.0)\n",
      "Requirement already satisfied: brotli in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from yt_dlp) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3,>=2.31.0->yt_dlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3,>=2.31.0->yt_dlp) (3.4)\n",
      "Requirement already satisfied: google-api-python-client in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (2.119.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.15.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-python-client) (2.17.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.62.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lilysu/anaconda3/envs/condaenv/lib/python3.8/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install yt_dlp\n",
    "!pip install youtube-transcript-api\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a43a1119-5631-4e52-919d-e8244f1d11c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing YouTube search at 2024-02-24 18:05:41\n",
      "\n",
      "\n",
      "Function 'process_videos_and_store' was run at 2024-02-24 18:05:42 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] unable to extract initial player response; please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] 96iaZxKRmKg: Failed to parse JSON (caused by JSONDecodeError(\"Expecting value in '': line 1 column 1 (char 0)\")); please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] unable to extract player version; please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "WARNING: [youtube] 96iaZxKRmKg: Failed to parse JSON (caused by JSONDecodeError(\"Expecting value in '': line 1 column 1 (char 0)\")); please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n",
      "ERROR: [youtube] 96iaZxKRmKg: 96iaZxKRmKg: Failed to parse JSON (caused by JSONDecodeError(\"Expecting value in '': line 1 column 1 (char 0)\")); please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download subtitles for video ID: 96iaZxKRmKg: ERROR: [youtube] 96iaZxKRmKg: 96iaZxKRmKg: Failed to parse JSON (caused by JSONDecodeError(\"Expecting value in '': line 1 column 1 (char 0)\")); please report this issue on  https://github.com/yt-dlp/yt-dlp/issues?q= , filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U at 2024-02-24 18:05:43\n",
      "\n",
      "Transcript found in English.\n",
      "Saved subtitles to subtitles/uT6ASPy2Dbs.txt\n",
      "Attempt 1: Cannot get transcript for video ID: uT6ASPy2Dbs\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import NoTranscriptFound\n",
    "from decimal import Decimal\n",
    "import yt_dlp\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def current_timestamp():\n",
    "    \"\"\"Returns the current timestamp formatted for readability.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def get_env_variables():\n",
    "    \"\"\"Fetch all necessary configurations from environment variables.\"\"\"\n",
    "    return {\n",
    "        'DEVELOPER_KEY': os.getenv('DEVELOPER_KEY'),\n",
    "        'AWS_ACCESS_KEY_ID': os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        'AWS_SECRET_ACCESS_KEY': os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "        'SEARCH_CACHE_TABLE': os.getenv('SEARCH_CACHE_TABLE'),\n",
    "        'RESULTS_TABLE_NAME': os.getenv('RESULTS_TABLE_NAME'),\n",
    "        'SEARCH_QUERY': os.getenv('SEARCH_QUERY'),\n",
    "        'MAX_RESULTS': int(os.getenv('MAX_RESULTS', 1)),\n",
    "        'ORDER': os.getenv('ORDER', 'viewCount'),\n",
    "        'VIDEO_DURATION': os.getenv('VIDEO_DURATION', 'medium'),\n",
    "        'PUBLISHED_AFTER': os.getenv('PUBLISHED_AFTER', '2010-01-01T00:00:00Z'),\n",
    "        'PUBLISHED_BEFORE': os.getenv('PUBLISHED_BEFORE', '2024-12-31T23:59:59Z'),\n",
    "        'RELEVANCE_LANGUAGE': os.getenv('RELEVANCE_LANGUAGE', 'en'),\n",
    "        # 'VIDEO_CATEGORY_ID': os.getenv('VIDEO_CATEGORY_ID', '10'),\n",
    "        'VECTARA_KEY':os.getenv('VECTARA_KEY'),\n",
    "        'AWS_REGION': os.getenv('AWS_REGION')  # Add AWS region to the environment variables\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def youtube_search_all_videos(options):\n",
    "    \"\"\"Perform a search on YouTube Data APIand return all videos based on the options, with caching and paging.\"\"\"\n",
    "    print(f\"Performing YouTube search at {current_timestamp()}\\n\")\n",
    "\n",
    "    developer_key = options['DEVELOPER_KEY']\n",
    "    # CREATE YOUTUBE OBJECT calling the youtbue data api \n",
    "    youtube = build('youtube', 'v3', developerKey=developer_key)\n",
    "    \n",
    "    all_videos = []\n",
    "    page_token = None\n",
    "    max_iterations = 2  # Adjust based on how many pages you want to retrieve (50 results max with 25 per page)\n",
    "\n",
    "    try:\n",
    "        for _ in range(max_iterations):\n",
    "            search_response = youtube.search().list(\n",
    "                q=options['SEARCH_QUERY'],\n",
    "                part='id,snippet',\n",
    "                maxResults=options['MAX_RESULTS'],\n",
    "                order=options['ORDER'],\n",
    "                type='video',\n",
    "                videoDuration=options['VIDEO_DURATION'],\n",
    "                publishedAfter=options['PUBLISHED_AFTER'],\n",
    "                publishedBefore=options['PUBLISHED_BEFORE'],\n",
    "                relevanceLanguage=options['RELEVANCE_LANGUAGE'],\n",
    "                # videoCategoryId=options['VIDEO_CATEGORY_ID'], # When there is a category ID assigned, if number 10, then results are mostly music.\n",
    "                pageToken=page_token\n",
    "            ).execute()\n",
    "\n",
    "            all_videos.extend(search_response.get('items', []))\n",
    "    \n",
    "            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]\n",
    "    \n",
    "            # Retrieve additional details for each video by their IDs, excluding restricted parts\n",
    "            if video_ids:\n",
    "                details_response = youtube.videos().list(\n",
    "                    id=','.join(video_ids),\n",
    "                    part='contentDetails,statistics,status,topicDetails,recordingDetails'\n",
    "                ).execute()\n",
    "    \n",
    "                all_videos.extend(details_response.get('items', []))\n",
    "    \n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error occurred: {e.resp.status} {e.content} at {current_timestamp()}\\n\")\n",
    "    return all_videos\n",
    "\n",
    "def download_subtitles_dlp(video_id):\n",
    "    \"\"\"Download subtitles for a given YouTube video ID.\"\"\"\n",
    "    video_url = f'https://www.youtube.com/watch?v={video_id}'\n",
    "    ydl_opts = {\n",
    "        'writeautomaticsub': True,\n",
    "        'subtitleslangs': ['en'],\n",
    "        'skip_download': True,\n",
    "        'outtmpl': f'subtitles/{video_id}.%(ext)s',\n",
    "        'quiet': True\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([video_url])\n",
    "            subtitle_file = f'subtitles/{video_id}.en.vtt'\n",
    "            if os.path.exists(subtitle_file):\n",
    "                with open(subtitle_file, 'r', encoding='utf-8') as file:\n",
    "                    subtitle_text = file.read()\n",
    "                    print(f\"Downloaded video subtitles for video ID: {video_id} into the file {subtitle_file} at {current_timestamp()}\\n\")\n",
    "                return subtitle_file, subtitle_text\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download subtitles for video ID: {video_id}: {e} at {current_timestamp()}\\n\")\n",
    "        return None, None\n",
    "\n",
    "def get_transcript_ytapi(video_id):\n",
    "    try:\n",
    "        # Attempt to fetch the transcript for the video in English\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "        print(\"Transcript found in English.\")\n",
    "        return transcript\n",
    "    except NoTranscriptFound as e:\n",
    "        print(\"English transcript not found. Attempting to list available languages...\")\n",
    "        try:\n",
    "            # Fetching the list of available transcript languages for the video\n",
    "            available_langs = YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "            manually_created = [lang.language_code for lang in available_langs.manually_created()]\n",
    "            generated = [lang.language_code for lang in available_langs.generated()]\n",
    "            print(\"Manually created transcripts available in: \", manually_created)\n",
    "            print(\"Auto-generated transcripts available in: \", generated)\n",
    "        except Exception as ex:\n",
    "            print(\"Failed to fetch available languages due to: \", ex)\n",
    "\n",
    "# def send_to_vectara(options, vectara, video_details):\n",
    "#     \"\"\"Send Youtube Data API results on the video and subtitle transcripts to vectara.\"\"\"\n",
    "#     try:\n",
    "#         # Convert float values to decimals\n",
    "#         video_details_decimal = convert_floats_to_decimals(video_details)\n",
    "\n",
    "#         results_table_name = options['RESULTS_TABLE_NAME']\n",
    "#         results_table = vectara.Table(results_table_name)\n",
    "#         response = results_table.put_item(Item=video_details_decimal)\n",
    "#         print(f\"Successfully inserted API results and converted transcripts into vectara at {current_timestamp()}\\n\", response)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error inserting into vectara: {e}\")\n",
    "\n",
    "def process_videos_and_store(data, parent_key='', result_dict={}):\n",
    "    \"\"\"Process each video from the search results and store them.\"\"\"\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            # Only pass the current key, not the entire path\n",
    "            extract_last_key_values_to_dict(v, k, result_dict)\n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data):\n",
    "            # For list items, keep the index in the key but also indicate it's an item in a list\n",
    "            current_key = f\"{parent_key}[{i}]\"\n",
    "            extract_last_key_values_to_dict(item, parent_key, result_dict)\n",
    "    else:\n",
    "        # Directly use parent_key as the final key\n",
    "        result_dict[parent_key] = data\n",
    "    \n",
    "    # send_to_vectara(options, vectara, result_dict)\n",
    "    print(f\"\\nFunction 'process_videos_and_store' was run at {current_timestamp()} \\n\")\n",
    "    return result_dict\n",
    "\n",
    "def upload_video_metadata(video_id, api_key, doc_metadata=None, extract_document=False):\n",
    "    \"\"\"\n",
    "    Upload video metadata to the specified endpoint with additional metadata.\n",
    "    \"\"\"\n",
    "    url = 'https://api.vectara.io/v1/upload'\n",
    "    params = {\n",
    "        'c': 2441028590,  # Using video_id as customer_id\n",
    "        'o': 2,  # Using video_id as corpus_id\n",
    "    }\n",
    "    if extract_document:\n",
    "        params['d'] = 'true'\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': 'application/txt',\n",
    "        'x-api-key': api_key,\n",
    "        # 'Authorization': f'Bearer {jwt_token}',\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    if doc_metadata:\n",
    "        data['doc_metadata'] = json.dumps(doc_metadata)\n",
    "    \n",
    "    file_path = f'{video_id}.json'  # File path using video_id with .json extension\n",
    "    \n",
    "    # Assuming the metadata file already exists, otherwise, you need to create it\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params, headers=headers, files=files, data=data)\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload video metadata for video ID: {video_id}: {e}\")\n",
    "    finally:\n",
    "        files['file'].close()\n",
    "\n",
    "def main():\n",
    "    # Load the .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Ensure the subtitles directory exists\n",
    "    os.makedirs('subtitles', exist_ok=True)\n",
    "    \n",
    "    # Get environment variables\n",
    "    options = get_env_variables()\n",
    "        \n",
    "    # Only proceed if vectara was successfully configured\n",
    "    if options:\n",
    "        all_videos = youtube_search_all_videos(options)\n",
    "        \n",
    "        # Only proceed if videos were successfully retrieved\n",
    "        if all_videos:\n",
    "            result_dict = process_videos_and_store(all_videos, parent_key='', result_dict={})\n",
    "\n",
    "            video_id = result_dict[\"videoId\"]\n",
    "            subtitle_file, subtitle_text = download_subtitles_dlp(video_id)\n",
    "            \n",
    "            # Initialize attempt counter\n",
    "            attempts = 0\n",
    "            max_attempts = 10\n",
    "            subtitle_text = None\n",
    "            \n",
    "            # Loop up to 10 times until subtitles are found\n",
    "            # while attempts < max_attempts and not subtitle_text:\n",
    "            if not subtitle_text:\n",
    "                try:\n",
    "                    video_id = 'uT6ASPy2Dbs'\n",
    "                    subtitle_text = get_transcript_ytapi(video_id)\n",
    "                    if subtitle_text:\n",
    "                        result_dict['subtitles'] = subtitle_text\n",
    "                        # Save to a file named \"{video_id}.txt\"\n",
    "                        file_path = os.path.join('subtitles', f\"{video_id}.txt\")\n",
    "                        with open(file_path, 'w') as file:\n",
    "                            # Format and write the result_dict as a string if necessary\n",
    "                            file.write(json.dumps(result_dict, indent=4))\n",
    "                        print(f\"Saved subtitles to {file_path}\")\n",
    "\n",
    "                        api_key = options['VECTARA_KEY']\n",
    "                        doc_metadata = {\"title\": result_dict[\"title\"], \"description\": result_dict[\"description\"]}\n",
    "                        # Assuming the upload_video_metadata function is defined elsewhere\n",
    "                        upload_video_metadata(video_id, api_key, doc_metadata=doc_metadata, extract_document=True)\n",
    "                        # break  # Exit the loop if subtitles are found\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempts + 1}: Cannot get transcript for video ID: {video_id}\")\n",
    "                attempts += 1\n",
    "\n",
    "            if not subtitle_text:\n",
    "                print(f\"Failed to retrieve subtitles after {max_attempts} attempts.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve videos at {current_timestamp()}. Exiting...\\n\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve environment variables at {current_timestamp()}. Exiting...\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa01f664-039f-42a6-8f1a-102325886ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to upload video metadata for video ID: uT6ASPy2Dbs: Invalid leading whitespace, reserved character(s), or returncharacter(s) in header name: 'Content-Type: multipart/form-dataAccept'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def upload_video_metadata(video_id, api_key, doc_metadata=None, extract_document=False):\n",
    "    \"\"\"\n",
    "    Upload video metadata to the specified endpoint with additional metadata.\n",
    "    \"\"\"\n",
    "    url = 'https://api.vectara.io/v1/upload'\n",
    "    params = {\n",
    "        'c': 2441028590,  # Using video_id as customer_id\n",
    "        'o': 2,  # Using video_id as corpus_id\n",
    "    }\n",
    "    if extract_document:\n",
    "        params['d'] = 'true'\n",
    "    \n",
    "    headers = {\n",
    "        'Content-Type: multipart/form-data'\n",
    "        'Accept': 'application/json',\n",
    "        'x-api-key': api_key,\n",
    "        # 'Authorization': f'Bearer {jwt_token}',\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    if doc_metadata:\n",
    "        data['doc_metadata'] = json.dumps(doc_metadata)\n",
    "    \n",
    "    file_path = f'{video_id}.json'  # File path using video_id with .json extension\n",
    "    \n",
    "    # Assuming the metadata file already exists, otherwise, you need to create it\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb'),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, params=params, headers=headers, files=files, data=data)\n",
    "        print(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload video metadata for video ID: {video_id}: {e}\")\n",
    "    finally:\n",
    "        files['file'].close()\n",
    "\n",
    "# Example usage\n",
    "video_id = 'uT6ASPy2Dbs'\n",
    "api_key = 'zut_kX8j7lr5zFJjUnhAf6XHHD83xQT3PUGhQyEBrQ'\n",
    "# jwt_token = 'your_jwt_token_here'\n",
    "doc_metadata = {\"title\": \"Sample Video Title\", \"description\": \"Sample video description\"}\n",
    "\n",
    "upload_video_metadata(video_id, api_key, doc_metadata=doc_metadata, extract_document=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cf8ec5ba-8198-43e5-a08d-0df727c4247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 400\n",
      "{\"httpCode\":400,\"internalCode\":3,\"details\":\"Request entity too large.\",\"status\":{}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def upload_to_vectara(api_key, payload={}, additional_headers=None):\n",
    "    \"\"\"\n",
    "    Upload data to the Vectara API.\n",
    "\n",
    "    :param api_key: API key for authentication.\n",
    "    :param payload: The payload to be uploaded.\n",
    "    :param additional_headers: Any additional headers to be included in the request.\n",
    "    \"\"\"\n",
    "    url = \"https://api.vectara.io/v1/upload\"\n",
    "\n",
    "    # Default headers\n",
    "    headers = {\n",
    "        'Content-Type': 'multipart/form-data',\n",
    "        'Accept': 'application/json',\n",
    "        'x-api-key': api_key,\n",
    "        # 'file=@\"/Users/lilysu/git/PatternPivot/base.txt\"',\n",
    "    }\n",
    "    \n",
    "    # If there are any additional headers, update the default headers with them\n",
    "    if additional_headers:\n",
    "        headers.update(additional_headers)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        print(\"Response Status Code:\", response.status_code)\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage of the function\n",
    "api_key = 'zut_kX8j7lr5zFJjUnhAf6XHHD83xQT3PUGhQyEBrQ'  # Replace <API_KEY_VALUE> with your actual API key\n",
    "payload = {}  # Your payload here\n",
    "additional_headers = None  # Any additional headers you want to include\n",
    "\n",
    "response_text = upload_to_vectara(api_key, payload, additional_headers)\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7a56818-5c1c-4f9b-98b1-e7d1c273dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "There are many things to be thankful for in life. Today is an opportunity to love and grow [1]. Feel grateful for your life and trust that everything is happening for your greatest good [2]. You are exactly where you're meant to be, and each new day is a chance to shine as your highest self [3]. Embrace the beauty and meaning in your life, and welcome the abundance flowing into your life with gratitude [4]. Choose joy, peace, balance, and light. Opt for love over fear and affirm your safety [5].\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.vectara.io/v1/query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "  \"query\": [\n",
    "    {\n",
    "      \"query\": \"What is there to be thankful for?\",\n",
    "      \"start\": 0,\n",
    "      \"numResults\": 10,\n",
    "      \"contextConfig\": {\n",
    "        \"sentencesBefore\": 2,\n",
    "        \"sentencesAfter\": 2,\n",
    "        \"startTag\": \"<b>\",\n",
    "        \"endTag\": \"</b>\"\n",
    "      },\n",
    "      \"corpusKey\": [\n",
    "        {\n",
    "          \"customerId\": 2441028590,\n",
    "          \"corpusId\": 2,\n",
    "          \"semantics\": \"DEFAULT\",\n",
    "          \"lexicalInterpolationConfig\": {\n",
    "            \"lambda\": 0\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"summary\": [\n",
    "        {\n",
    "          \"maxSummarizedResults\": 5,\n",
    "          \"responseLang\": \"eng\",\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "})\n",
    "\n",
    "\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Accept': 'application/json',\n",
    "  'customer-id': '2441028590',\n",
    "  'x-api-key': 'zut_kX8j7lr5zFJjUnhAf6XHHD83xQT3PUGhQyEBrQ'\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "# print(response.text)\n",
    "\n",
    "# Check if the response status code is 200 (OK) to ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        # Parse the response JSON content\n",
    "        data = response.json()\n",
    "\n",
    "        # Navigate through the nested structure to extract the summary\n",
    "        summaries = data['responseSet'][0]['summary']\n",
    "        \n",
    "        # Assuming there's at least one summary in the list and extracting the first one for simplicity\n",
    "        summary_text = summaries[0]['text'] if summaries else 'No summary found.'\n",
    "\n",
    "        # Print the formatted summary\n",
    "        print(\"Summary:\\n\")\n",
    "        print(summary_text)\n",
    "    except KeyError as e:\n",
    "        print(f\"Key not found in response: {e}\")\n",
    "    except IndexError as e:\n",
    "        print(f\"Index error, might be due to unexpected response structure: {e}\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}, Response text: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64aeff7-bc93-4d7d-a0e2-58cc3a50fbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
